title: How to launch a 1001 Node Crate Cluster on Microsoft Azure Infrastructure
author: Chris Ward
description: We show how to scale your Crate cluster for billions of records and beyond as simple as possible by launching a 1001 node cluster on Microsoft Azure infrastructure.
created: 2016-03-30
status: publish
post_type: post
tags: azure, docker, cloud
category: developernews

# How to launch a 1001 Node Crate Cluster on Microsoft Azure Infrastructure

[Crate.io](https://crate.io/) is striving to create the most scalable database in our industry;
we aim to make scaling your data from millions to billions of records and beyond, as simple as
possible.

We have clients who write and query gigabytes of data a minute, store terabytes a day and love the
performance and stability that Crate offers. But the question always remained at the back of our
minds: when taken to the extreme, what is Crate really capable of?

We specifically wanted to:

- Deploy a Crate cluster of 1001 nodes.
- Examine the overhead added by the cluster management.
- See at what speeds the cluster could write data.
- Assess the performance of different configurations of the cluster.

Microsoft Azure generously lent us the use of 8000 cores as the infrastructure for this test and
engineer time to guide us through problems and questions we might have.

Microsoft wanted to show that their Azure platform was reliable and capable enough for a project
that would scale rapidly and so resource intensive.


## Plan

While discussing the approach of setting up a 1001 node Crate cluster, we immediately thought about
using [Docker Swarm](https://www.docker.com/products/docker-swarm) to launch Crate. However, we
also wanted to see how the conventional deployment, using regular packages, would work out.

### Docker Swarm Setup

Our initial approach was to start the Crate cluster in a containerized environment using
[Docker](https://www.docker.com), [Docker Swarm](https://www.docker.com/products/docker-swarm)
for scheduling Crate instances and [Consul](https://www.consul.io/) for node discovery in a Swarm
cluster. The solution has its pros and cons. Scheduling Crate nodes, and being able to change
settings in the Crate cluster with minimal effort (in order to repeat certain experiments, by any
means), is the strongest side of the chosen approach. On the other hand, it adds complexity to the
overhead in the Crate cluster setup. For example, Consul and Swarm cluster must be deployed first.
Moreover, the elaborate setup results in higher network load and additional connections introduced
by backend clusters.

### Conventional Crate Setup

For this reason, we wanted to be certain that the high network traffic was not due to Crate, so
our next plan was to go back to basics and eliminate other potential sources of noise. Therefore,
we decided to use the Crate Debian package to setup the cluster and conduct test using as simple as
possible approach for spinning up the cluster.


## Azure Setup

We aimed to create a 1001 node Crate cluster ready for data import and manipulation. Setting up such
a cluster on any kind of infrastructure requires planning and whilst cloud hosts simplify this
process, understanding how to translate your requirements into their paradigms requires research
and experimentation.

Azure has several concepts that we needed to understand and make use of to get our cluster started:

- **Resource Group**: A 'container' that holds related resources of all types for an application.
- **Storage Account**: A data storage solution allocated to you that can contain Blob, Table, Queue,
    and File data. Can be stored on SSDs or HDDs.
- **Virtual Network (vnet)**: A representation of a network in the cloud, where you can define DHCP
    address blocks, DNS settings, security policies, and routing.
- **Subnet**: Further segmentation for virtual networks.
- **Virtual Machine (VM)**: Represents a particular operating system or application stack.
- **Network Interface (NIC)**: Represents a network interface that can be associated to one or more
    virtual machines (VM).
- **Network Security Group (NSG)**: Contains a list of rules that allow or deny network traffic to
    your VMs in a Virtual Network. Can be associated with subnets or individual VM instances within
    that subnet.

### Limits

These are some platform specific limitations that influenced our architecture decisions:

- 8000 CPU cores
- 10000 VMs
- 100 VMs in single availability set
- 800 resources in single resource group
- 4096 IP addresses in a virtual network (VNet)

### ARM Templates

[Azure Templates]
(https://azure.microsoft.com/en-us/documentation/articles/resource-group-authoring-templates/), a
json formatted file that provisions all the resources and commands to be run for your application
in a single operation. These templates are used in conjunction with the Azure command line tool to
trigger a complex cluster deployment quickly.

An ARM template has a simple structure:

```json
{
   "$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
   "contentVersion": "",
   "parameters": {  },
   "variables": {  },
   "resources": [  ],
   "outputs": {  }
}
```

It is worth highlighting the resource dependency management in ARM templates, which provide a
cohesive template structure and reliable deployment of resources.


### Deployment Setup

Taking into account limitations of the Azure platform, especially number of resources per single
resource group and number of virtual machines in single availability set and convenient way for
possible redeploying of resources, we came to a conclusion to split the resource setup into logical
self contained units defined by their function:

    * 1 Network infrastructure group
    * 1 Docker Swarm managers group
    * 1 Crate masters group
    * 11 Scale unit groups
    * 1 Ganglia monitoring group

#### Network Infrastructure

As a fundamental component of the overall deployment architecture, the resource group units serve the
[network resource](https://github.com/crate/p-azure/blob/master/arm-templates/network.deploy.json)
group. The network ARM template defines only three types of resources to be deployed, they are:

    * Virtual Network
    * Network Security Group
    * Public IP addresses used for Swarm Manager VMs

It is also possible to reference resources by ID from other groups by providing its type and an
explicit path to it:

```bash
resourceId ([resourceGroupName], resourceType, resourceName1, [resourceName2]...)
```

For instance, we reference the virtual network from the network resource group in each of ARM
template as following:

```json
    ...
    "virtualNetworkName": "vnet1k1",
    "resourceGroupNetworkName": "1k1network",
    ...
    "vnetID": "[resourceId(variables('resourceGroupNetworkName'), 'Microsoft.Network/virtualNetworks'
        , variables('virtualNetworkName'))]",
    ...
```

#### Docker Swarm managers

Docker Swarm managers and Consul agents run in the dedicated [Docker Swarm managers group]
(https://github.com/crate/p-azure/blob/master/arm-templates/docker/swarmmanagers.json).

    * 3 Network Interfaces
    * 3 Virtual Machine
    * 6 Microsoft Azure extensions
    * 1 Availability set
    * 1 Storage account

#### Crate nodes

All in all, to have 1001 Crate data nodes in the cluster,
[11 scale unit](https://github.com/crate/p-azure/blob/master/arm-templates/docker/scaleunit.json)
groups must be deployed. With an idea of keeping the template as simple as possible, while
satisfying existing constraints, the following structure of the template was proposed:

    * 100 Network Interfaces
    * 100 Virtual Machine
    * 200 Microsoft Azure extensions
    * 1 Availability set
    * 5 Storage accounts

#### Crate masters

The only difference between the resources in scale and master units is the type of virtual machines
and their number. Therefore, we use the scale unit
[ARM template](https://github.com/crate/p-azure/blob/master/arm-templates/docker/scaleunit.json)
with slightly modified [configuration]
(https://github.com/crate/p-azure/blob/master/arm-templates/docker masterunit.parameters.json)
effecting the address space, number of storage account and VM type.

#### Ganglia monitoring

Yet another resource group is
[Ganglia monitoring](https://github.com/crate/p-azure/tree/master/arm-templates/ganglia). For each
scale and master unit we assign a dedicated VM to collect and store aggregated metrics into a
storage engine like [RRD](http://www.fromdual.com/round-robin-database-storage-engine).

#### Deployment

Prepared [ARM templates](https://github.com/crate/p-azure/tree/master/arm-templates) are
deployed using Azure CLI. Before the template deployment a resource group in a certain location
must be created.


```bash
azure group create -n <resourcegroup> -l <location>
azure group deployment create \
    -f azuredeploy.json \
    -e azuredeploy.parameters.json \
    -g <resourcegroup>
```


## Dimensioning

For those interested and keeping count, here's the total Azure VMs we made use of and
how [Azure references](https://azure.microsoft.com/en-us/documentation/articles/
virtual-machines-size-specs/#standard-tier-dv2-series) them:

- 12x **Ganglia VMs**: _Standard_D4_v2_ (8 cores, 28GB RAM, 400GB local SSD)
- 3x **Swarm Manager VMs**: _Standard_D14_v2_ (16 cores, 112GB RAM, 800GB local SSD)
- 3x **Crate Master VMs**: _Standard_D5_v2_ (16 cores, 56GB RAM, 800GB local SSD)
- 1100x **Crate Node VMs**: _Standard_D12_v2_ (4 cores, 28GB RAM, 200GB local SSD)

That all adds up to a gigantic **4448** cores and **30968GB** RAM, **just** for the Crate instances.


## Conclusion

Deploying 1k1 Crate nodes on Microsoft Azure was an experiment with many aspects: first formal
attempt to push Crate to its limits in large deployments, first large-scale deployment on Azure.
In addition to learning more about how Crate behaves at scale, this was also an important
opportunity for Crate.io to work alongside our partners at Microsoft Azure and Docker.

This will not be the last time we run an experiment like this. At Crate.io we shall continue to
push the Crate technology to its limits in order to continually improve and to ensure we continue
to scale horizontally, linearly with performance improves with every increase in scale.
